{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一. spacy nlp处理包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\")\n",
    "doc = nlp(\"The big grey dog ate all of the chocolate, but fortunately he wasn't sick!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(spacy.lang.en.English, spacy.tokens.doc.Doc)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp), type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'big',\n",
       " 'grey',\n",
       " 'dog',\n",
       " 'ate',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'chocolate,',\n",
       " 'but',\n",
       " 'fortunately',\n",
       " 'he',\n",
       " \"wasn't\",\n",
       " 'sick!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分词\n",
    "doc.text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'big',\n",
       " 'grey',\n",
       " 'dog',\n",
       " 'ate',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'chocolate',\n",
       " ',',\n",
       " 'but',\n",
       " 'fortunately',\n",
       " 'he',\n",
       " 'was',\n",
       " \"n't\",\n",
       " 'sick',\n",
       " '!']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.orth_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(The, 'The', 5059648917813135842),\n",
       " (big, 'big', 15511632813958231649),\n",
       " (grey, 'grey', 10475807793332549289),\n",
       " (dog, 'dog', 7562983679033046312),\n",
       " (ate, 'ate', 10806788082624814911),\n",
       " (all, 'all', 13409319323822384369),\n",
       " (of, 'of', 886050111519832510),\n",
       " (the, 'the', 7425985699627899538),\n",
       " (chocolate, 'chocolate', 10946593968795032542),\n",
       " (,, ',', 2593208677638477497),\n",
       " (but, 'but', 14560795576765492085),\n",
       " (fortunately, 'fortunately', 13851269277375979931),\n",
       " (he, 'he', 1655312771067108281),\n",
       " (was, 'was', 9921686513378912864),\n",
       " (n't, \"n't\", 2043519015752540944),\n",
       " (sick, 'sick', 14841597609857081305),\n",
       " (!, '!', 17494803046312582752)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(token, token.orth_, token.orth) for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 词干提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['practice', 'practice', 'practicing']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 词干提取\n",
    "practice = \"practice practiced practicing\"\n",
    "nlp_practice = nlp(practice)\n",
    "[word.lemma_ for word in nlp_practice]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.词性标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Conor, 'NNP'),\n",
       " ('s, 'POS'),\n",
       " (dog, 'NN'),\n",
       " ('s, 'POS'),\n",
       " (toy, 'NN'),\n",
       " (was, 'VBD'),\n",
       " (hidden, 'VBN'),\n",
       " (under, 'IN'),\n",
       " (the, 'DT'),\n",
       " (man, 'NN'),\n",
       " ('s, 'POS'),\n",
       " (sofa, 'NN'),\n",
       " (in, 'IN'),\n",
       " (the, 'DT'),\n",
       " (woman, 'NN'),\n",
       " ('s, 'POS'),\n",
       " (house, 'NN')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 词性标注 POS\n",
    "doc2 = nlp(\"Conor's dog's toy was hidden under the man's sofa in the woman's house\")\n",
    "pos_tags = [(i, i.tag_) for i in doc2]\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Conor, dog), (dog, toy), (man, sofa), (woman, house)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "owners_possessions = [(i[0].nbor(-1), i[0].nbor(1)) for i in pos_tags if i[1] == 'POS']\n",
    "owners_possessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 实体识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wiki_obama = \"\"\"Barack Obama is an America polician who served as the 44th Persident of the United States\n",
    "from 2009 to 2017. He is the first Africa American to have served as the Persident, as well as the first born \n",
    "outside the contiguous United States.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Barack Obama, 'PERSON', 378),\n",
       " (America, 'GPE', 382),\n",
       " (44th, 'ORDINAL', 393),\n",
       " (2009 to 2017, 'DATE', 388),\n",
       " (first, 'ORDINAL', 393),\n",
       " (Africa American, 'NORP', 379),\n",
       " (Persident, 'FAC', 9191306739292312949),\n",
       " (first, 'ORDINAL', 393),\n",
       " (, 'GPE', 382),\n",
       " (United States, 'GPE', 382)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_obama = nlp(wiki_obama)\n",
    "[(i, i.label_, i.label) for i in nlp_obama.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence number 1: Barack Obama is an America polician who served as the 44th Persident of the United States\n",
      "from 2009 to 2017.\n",
      "Sentence number 2: He is the first Africa American to have served as the Persident, as well as the first born \n",
      "outside the contiguous United States.\n"
     ]
    }
   ],
   "source": [
    "for ix, sent in enumerate(nlp_obama.sents, 1):\n",
    "    print(\"Sentence number {ix}: {sent}\".format(ix=ix, sent=sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "1. 分词\n",
    "---\n",
    "Ltp与结巴jieba分词，jieba核心使用NShort算法\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Ltp 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyltp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pyltp import Segmentor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "cws.model 分词模型\n",
    "pos.model 词性标注模型\n",
    "ner.model 命名体识别模型\n",
    "parser.model 依存句法分析模型\n",
    "pisrl.model 语义角色标注模型\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在 | 包含 | 问题 | 的 | 所有 | 解 | 的 | 解 | 空间 | 树 | 中 | ， | 按照 | 深度 | 优先 | 搜索 | 的 | 策略 | ， | 从 | 根节点 | 出发 | 深度 | 探索 | 解 | 空间 | 树 | 。\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/Users/chenhui/ltp_data_v3.4.0/cws.model\" # 分词模型库\n",
    "segmentor = Segmentor()\n",
    "segmentor.load(model_path)\n",
    "words = segmentor.segment(\"在包含问题的所有解的解空间树中，按照深度优先搜索的策略，从根节点出发深度探索解空间树。\")\n",
    "print(' | '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'在 | 包含 | 问题 | 的 | 所有 | 解 | 的 | 解空间 | 树 | 中 | ， | 按照 | 深度优先 | 搜索 | 的 | 策略 | ， | 从 | 根节点 | 出发 | 深度 | 探索 | 解空间 | 树 | 。'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_sent = ' | '.join(words)\n",
    "postdict = {\"解 | 空间\": \"解空间\", \"深度 | 优先\":\"深度优先\"}\n",
    "for key in postdict:\n",
    "    seg_sent = seg_sent.replace(key, postdict[key])\n",
    "seg_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 加入用户词典\n",
    "segmentor.load_with_lexicon(model_path, fulluserdict.txt)\n",
    "words = segmentor.segment(\"在包含问题的所有解的解空间树中，按照深度优先搜索的策略，从根节点出发深度探索解空间树。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 结巴分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import jieba\n",
    "sent = \"在包含问题的所有解的解空间树中，按照深度优先搜索的策略，从根节点出发深度探索解空间树。\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 结巴分词-全模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/kx/75q0msjd2xv_rzkypkrw7f7r0000gn/T/jieba.cache\n",
      "Loading model cost 0.923 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在 | 包含 | 问题 | 的 | 所有 | 解 | 的 | 解空 | 空间 | 树 | 中 |  |  | 按照 | 深度 | 优先 | 搜索 | 的 | 策略 |  |  | 从 | 根 | 节点 | 点出 | 出发 | 深度 | 探索 | 索解 | 解空 | 空间 | 树 |  | \n"
     ]
    }
   ],
   "source": [
    "wordlist = jieba.cut(sent, cut_all= True)\n",
    "print(\" | \".join(wordlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 结巴分词-精确切分 (类似于Ltp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在 | 包含 | 问题 | 的 | 所有 | 解 | 的 | 解 | 空间 | 树中 | ， | 按照 | 深度 | 优先 | 搜索 | 的 | 策略 | ， | 从根 | 节点 | 出发 | 深度 | 探索 | 解 | 空间 | 树 | 。\n"
     ]
    }
   ],
   "source": [
    "wordlist = jieba.cut(sent)  # cut_all = False\n",
    "print(\" | \".join(wordlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 结巴分词-搜索引擎模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在 | 包含 | 问题 | 的 | 所有 | 解 | 的 | 解 | 空间 | 树中 | ， | 按照 | 深度 | 优先 | 搜索 | 的 | 策略 | ， | 从根 | 节点 | 出发 | 深度 | 探索 | 解 | 空间 | 树 | 。\n"
     ]
    }
   ],
   "source": [
    "wordlist = jieba.cut_for_search(sent)\n",
    "print(\" | \".join(wordlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义用户词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n格式：一个词占一行，每行分三部分：词语，词频，词性\\n  解空间 5 n\\n  解空间树 5 n\\n  \\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "格式：一个词占一行，每行分三部分：词语，词频，词性\n",
    "  解空间 5 n\n",
    "  解空间树 5 n\n",
    "  根节点 5 n\n",
    "  深度优先 5 n\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用用户词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在 | 包含 | 问题 | 的 | 所有 | 解 | 的 | 解空间树 | 中 | ， | 按照 | 深度优先 | 搜索 | 的 | 策略 | ， | 从 | 根节点 | 出发 | 深度 | 探索 | 解空间树 | 。\n"
     ]
    }
   ],
   "source": [
    "jieba.load_userdict('Userdict.txt')\n",
    "wordlist = jieba.cut(sent)\n",
    "print(\" | \".join(wordlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用户词典起到了作用，会和内置词典相互博弈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "2. 词性标注\n",
    "---\n",
    "Ltp与Stanford，中文词性标注大多数使用HMM（隐马尔科夫）和最大熵，如结巴；CRF有更高的精度，如Ltp\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Ltp词性标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pyltp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent = \"在 包含 问题 的 所有 解 的 解空间树 中 ， 按照 深度优先 搜索 的 策略 ， 从 根节点 出发 深度 探索 解空间树 。\"\n",
    "words = sent.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在/p 包含/v 问题/n 的/u 所有/b 解/v 的/u 解空间树/n 中/nd ，/wp 按照/p 深度优先/d 搜索/v 的/u 策略/n ，/wp 从/p 根节点/n 出发/v 深度/n 探索/v 解空间树/n 。/wp "
     ]
    }
   ],
   "source": [
    "postagger = Postagger()\n",
    "postagger.load(\"/Users/chenhui/ltp_data_v3.4.0/pos.model\")\n",
    "postags = postagger.postag(words)\n",
    "for word, postag in zip(words, postags):\n",
    "    print(word + \"/\" + postag, end= \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Stanford词性标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 执行stanford PostTagger类来进行词性标注\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参见https://stanfordnlp.github.io/CoreNLP/cmdline.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在/P 包含/VV 问题/NN 的/DEC 所有/DT 解/VV 的/DEC 解空间树/NN 中/LC ，/PU 按照/P 深度优先/NN 搜索/NN 的/DEC 策略/NN ，/PU 从/P 根节点/NN 出发/VV 深度/JJ 探索/NN 解空间树/VV 。/PU\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import stanford\n",
    "root = \"/Users/chenhui/stanford-corenlp/\"\n",
    "modelpath = root+\"chinese-models/edu/stanford/nlp/models/pos-tagger/chinese-distsim/chinese-distsim.tagger\"\n",
    "st = stanford.StanfordPOSTagger(root, modelpath)\n",
    "seg_sent = \"在 包含 问题 的 所有 解 的 解空间树 中 ， 按照 深度优先 搜索 的 策略 ， 从 根节点 出发 深度 探索 解空间树 。\"\n",
    "taglist = st.tag(seg_sent)\n",
    "print(taglist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'java -Xmx1g -cp \"/Users/chenhui/stanford-corenlp/stanford-corenlp-3.9.1.jar:/Users/chenhui/stanford-corenlp/stanford-corenlp-3.9.1-models.jar:/Users/chenhui/stanford-corenlp/ejml-0.23.jar:/Users/chenhui/stanford-corenlp/javax.json.jar:/Users/chenhui/stanford-corenlp/jollyday.jar:/Users/chenhui/stanford-corenlp/joda-time.jar:/Users/chenhui/stanford-corenlp/protobuf.jar:/Users/chenhui/stanford-corenlp/slf4j-api.jar:/Users/chenhui/stanford-corenlp/slf4j-simple.jar:/Users/chenhui/stanford-corenlp/xom.jar\" edu.stanford.nlp.tagger.maxent.MaxentTagger -model \"/Users/chenhui/stanford-corenlp/chinese-models/edu/stanford/nlp/models/pos-tagger/chinese-distsim/chinese-distsim.tagger\" -tagSeparator /'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.cmdline # mac或者linux使用(:)冒号分隔class path，windows使用(;)分号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello Userdict.txt\n",
      "__init__.py\n",
      "__pycache__\n",
      "derby.log\n",
      "metastore_db\n",
      "nlp_beginer.ipynb\n",
      "stanford.py\n",
      "\n",
      "QQ已死有事请烧纸\n"
     ]
    }
   ],
   "source": [
    "import os  \n",
    "mystr=os.popen(\"ls\")  #popen与system可以执行指令,popen可以接受返回对象  \n",
    "mystr=mystr.read() #读取输出  \n",
    "print(\"hello\",mystr)  \n",
    "if mystr.find(\"QQ.exe\") !=-1:  \n",
    "    print(\"发现QQ\")  \n",
    "else:  \n",
    "    print(\"QQ已死有事请烧纸\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "3. 命名实体识别\n",
    "---\n",
    "Ltp与Stanford\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Ltp ner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pyltp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['欧洲',\n",
       " '东部',\n",
       " '的',\n",
       " '罗马尼亚',\n",
       " '，',\n",
       " '首都',\n",
       " '是',\n",
       " '布加勒斯特',\n",
       " '，',\n",
       " '也',\n",
       " '是',\n",
       " '一',\n",
       " '座',\n",
       " '世界性',\n",
       " '的',\n",
       " '城市',\n",
       " '。']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"欧洲 东部 的 罗马尼亚 ， 首都 是 布加勒斯特 ， 也 是 一 座 世界性 的 城市 。\"\n",
    "words = sent.split()\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "欧洲/ns/S-Ns 东部/nd/O 的/u/O 罗马尼亚/ns/S-Ns ，/wp/O 首都/n/O 是/v/O 布加勒斯特/ns/S-Ns ，/wp/O 也/d/O 是/v/O 一/m/O 座/q/O 世界性/n/O 的/u/O 城市/n/O 。/wp/O "
     ]
    }
   ],
   "source": [
    "# 词性标注\n",
    "postagger = Postagger()\n",
    "postagger.load(\"/Users/chenhui/ltp_data_v3.4.0/pos.model\")  # 导入词性标注模块（语法层面）\n",
    "postags = postagger.postag(words)\n",
    "\n",
    "# 命名体识别\n",
    "recognizer = NamedEntityRecognizer()\n",
    "recognizer.load(\"/Users/chenhui/ltp_data_v3.4.0/ner.model\")\n",
    "netags = recognizer.recognize(words, postags)\n",
    "\n",
    "for word, postag, netag in zip(words, postags, netags):\n",
    "    print(word+'/'+postag+'/'+netag, end=' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S-Ns',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-Ns',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'S-Ns',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(netags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Stanford 命名实体识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os \n",
    "import stanford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "root = \"/Users/chenhui/stanford-corenlp/\"\n",
    "modelpath = os.path.join(root, \"chinese-models/edu/stanford/nlp/models/ner/chinese.misc.distsim.crf.ser.gz\")\n",
    "st = stanford.StanfordNERTagger(root, modelpath)\n",
    "seg_sent = \"欧洲 东部 的 罗马尼亚 ， 首都 是 布加勒斯特 ， 也 是 一 座 世界性 的 城市 。\"\n",
    "taglist = st.tagfile(seg_sent, \"ner_test.txt\")\n",
    "print(taglist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(modelpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(st.tag(seg_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "4. 句法解析\n",
    "---\n",
    "Ltp句法依存树，Stanford：PCFG解析器，Shift-Reduce解析器，神经网络依存解析器\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Ltp句法依存树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tree import Tree  # 导入NLTK tree结构\n",
    "from nltk.grammar import DependencyGrammar\n",
    "from nltk.parse import *\n",
    "from pyltp import *\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t欧洲(ns)\tns\t2\tATT\n",
      "\t东部(nd)\tnd\t4\tATT\n",
      "\t的(u)\tu\t2\tRAD\n",
      "\t罗马尼亚(ns)\tns\t7\tSBV\n",
      "\t，(wp)\twp\t4\tWP\n",
      "\t首都(n)\tn\t7\tSBV\n",
      "\t是(v)\tv\t0\tROOT\n",
      "\t布加勒斯特(ns)\tns\t7\tVOB\n",
      "\t，(wp)\twp\t7\tWP\n",
      "\t也(d)\td\t11\tADV\n",
      "\t是(v)\tv\t7\tCOO\n",
      "\t一(m)\tm\t13\tATT\n",
      "\t座(q)\tq\t16\tATT\n",
      "\t世界性(n)\tn\t16\tATT\n",
      "\t的(u)\tu\t14\tRAD\n",
      "\t城市(n)\tn\t11\tVOB\n",
      "\t。(wp)\twp\t7\tWP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 分词\n",
    "words = \"罗马尼亚 的 首都 是 布加勒斯特 。 \".split()  # 分词\n",
    "words = \"欧洲 东部 的 罗马尼亚 ， 首都 是 布加勒斯特 ， 也 是 一 座 世界性 的 城市 。\".split()\n",
    "\n",
    "# 词性标注\n",
    "postagger = Postagger()\n",
    "postagger.load(\"/Users/chenhui/ltp_data_v3.4.0/pos.model\")\n",
    "postags = postagger.postag(words)  # 词性标注\n",
    "\n",
    "# 句法解析\n",
    "parser = Parser()\n",
    "parser.load(\"/Users/chenhui/ltp_data_v3.4.0/parser.model\")\n",
    "arcs = parser.parse(words, postags)  # 输入分词结果和词性标注\n",
    "arclen = len(arcs)\n",
    "conll = \"\"\n",
    "for i in range(arclen):  # 构建Conll标准的数据结构\n",
    "    if arcs[i].head == 0:\n",
    "        arcs[i].relation = \"ROOT\"\n",
    "    conll += \"\\t\"+words[i]+\"(\"+postags[i]+\")\" + \"\\t\"+postags[i]+ \"\\t\"+ str(arcs[i].head)+ \"\\t\" + arcs[i].relation+\"\\n\"\n",
    "print(conll)\n",
    "conlltree = DependencyGraph(conll)  # 转换为依存句法图\n",
    "tree = conlltree.tree()  # 构建树结构\n",
    "\n",
    "#tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 'ATT')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arcs[0].head, arcs[0].relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Stanford Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stanford短语树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys  \n",
    "import os  \n",
    "import nltk  \n",
    "from nltk.tree import Tree    #导入nltk tree结构  \n",
    "from stanford import *  \n",
    "  \n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf-8')  \n",
    "  \n",
    "# 配置环境变量  \n",
    "#os.environ['JAVA_HOME'] =   \n",
    "  \n",
    "root = \"stanford-corenlp/\"  \n",
    "modelpath = root + \"models/edu/stanford/nlp/models/lexparser/chinesePCFG.ser.gz\"  \n",
    "opttype = 'penn'  #滨州树库格式  \n",
    "parser = StanfordParser(modelpath, root, opttype)  \n",
    "result = parser.parse(\"罗马尼亚 的 首都 是 布加勒斯特 。\")  \n",
    "print result  \n",
    "  \n",
    "tree = Tree.fromstring(result)  \n",
    "tree.draw()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stanford句法依存树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-  \n",
    "import sys  \n",
    "import os  \n",
    "import nltk  \n",
    "from nltk.tree import Tree    #导入nltk tree结构  \n",
    "from stanford import *  \n",
    "\n",
    "# 配置环境变量  \n",
    "#os.environ['JAVA_HOME'] =   \n",
    "  \n",
    "root = \"stanford-corenlp/\"  \n",
    "modelpath = root + \"models/edu/stanford/nlp/models/lexparser/chinesePCFG.ser.gz\"  \n",
    "opttype = '<span style=\"color:#ff0000;\">typedDependencies</span>'  #   \n",
    "parser = StanfordParser(modelpath, root, opttype)  \n",
    "result = parser.parse(\"罗马尼亚 的 首都 是 布加勒斯特 。\")  \n",
    "print result  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "5. 语义角色标注SemanticRoleLabeling，SRL\n",
    "---\n",
    "Ltp\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "Python argument types in\n    SementicRoleLabeller.label(SementicRoleLabeller, VectorOfString, VectorOfString, VectorOfString, VectorOfParseResult)\ndid not match C++ signature:\n    label(SementicRoleLabeller {lvalue}, boost::python::list, boost::python::list, std::__1::vector<std::__1::pair<int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > >)\n    label(SementicRoleLabeller {lvalue}, boost::python::list, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > >, std::__1::vector<std::__1::pair<int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > >)\n    label(SementicRoleLabeller {lvalue}, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > >, boost::python::list, std::__1::vector<std::__1::pair<int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > >)\n    label(SementicRoleLabeller {lvalue}, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > >, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > >, std::__1::vector<std::__1::pair<int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > >)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-edffdca6a4fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mlabeller\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSementicRoleLabeller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mlabeller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODELDIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pisrl.model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mroles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabeller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marcs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#输出标注结果\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mArgumentError\u001b[0m: Python argument types in\n    SementicRoleLabeller.label(SementicRoleLabeller, VectorOfString, VectorOfString, VectorOfString, VectorOfParseResult)\ndid not match C++ signature:\n    label(SementicRoleLabeller {lvalue}, boost::python::list, boost::python::list, std::__1::vector<std::__1::pair<int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > >)\n    label(SementicRoleLabeller {lvalue}, boost::python::list, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > >, std::__1::vector<std::__1::pair<int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > >)\n    label(SementicRoleLabeller {lvalue}, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > >, boost::python::list, std::__1::vector<std::__1::pair<int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > >)\n    label(SementicRoleLabeller {lvalue}, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > >, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > >, std::__1::vector<std::__1::pair<int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > >)"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-  \n",
    "import sys  \n",
    "import os  \n",
    "from pyltp import *  \n",
    "  \n",
    "MODELDIR = \"/Users/chenhui/ltp_data_v3.4.0/\"  \n",
    "sentence = \"欧洲东部的罗马尼亚，首都是布加勒斯特，也是一座世界性的城市。\"  \n",
    "  \n",
    "segmentor = Segmentor()  \n",
    "segmentor.load(os.path.join(MODELDIR, \"cws.model\"))  \n",
    "words = segmentor.segment(sentence)  \n",
    "wordlist = list(words)  #从生成器变为列表元素  \n",
    "  \n",
    "postagger = Postagger()  \n",
    "postagger.load(os.path.join(MODELDIR, \"pos.model\"))  \n",
    "postags = postagger.postag(words)  \n",
    "  \n",
    "parser = Parser()  \n",
    "parser.load(os.path.join(MODELDIR, \"parser.model\"))  \n",
    "arcs = parser.parse(words, postags)  \n",
    "  \n",
    "recognizer = NamedEntityRecognizer()  \n",
    "recognizer.load(os.path.join(MODELDIR, \"ner.model\"))  \n",
    "netags = recognizer.recognize(words, postags)  \n",
    "  \n",
    "#语义角色标注  \n",
    "labeller = SementicRoleLabeller()  \n",
    "labeller.load(os.path.join(MODELDIR, \"pisrl.model\"))  \n",
    "roles = labeller.label(words, postags, netags, arcs)  \n",
    "  \n",
    "#输出标注结果  \n",
    "for role in roles:  \n",
    "    print('rel:', wordlist[role.index]) #谓词  \n",
    "    for arg in role.arguments:  \n",
    "        if arg.range.start != arg.range.end:  \n",
    "            print(arg.name, ' '.join(wordlist[arg.range.start:arg.range.end]) ) \n",
    "        else:  \n",
    "            print(arg.name,wordlist[arg.range.start])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
